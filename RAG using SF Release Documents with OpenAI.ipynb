{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in ./venv/lib/python3.14/site-packages (0.11.9)\n",
      "Requirement already satisfied: pypdf in ./venv/lib/python3.14/site-packages (6.6.0)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.14/site-packages (2.15.0)\n",
      "Requirement already satisfied: faiss-cpu in ./venv/lib/python3.14/site-packages (1.13.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.14/site-packages (2.4.1)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.14/site-packages (1.2.1)\n",
      "Requirement already satisfied: pdf2image in ./venv/lib/python3.14/site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.14/site-packages (12.1.0)\n",
      "Requirement already satisfied: pdfminer.six==20251230 in ./venv/lib/python3.14/site-packages (from pdfplumber) (20251230)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./venv/lib/python3.14/site-packages (from pdfplumber) (5.3.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./venv/lib/python3.14/site-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./venv/lib/python3.14/site-packages (from pdfminer.six==20251230->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.14/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.14/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.14/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.14/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.14/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.14/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.14/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.14/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.14/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.14/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in ./venv/lib/python3.14/site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.14/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Setup complete! OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and Dependencies\n",
    "# Install required packages\n",
    "%pip install pdfplumber pypdf openai faiss-cpu numpy python-dotenv pdf2image pillow\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import faiss\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"Setup complete! OpenAI client initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85b6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has extractable text: True\n",
      "✓ Using direct text extraction (much faster!)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check PDF for Embedded Text\n",
    "def check_pdf_has_text(pdf_path: str) -> bool:\n",
    "    \"\"\"Check if PDF contains extractable text.\"\"\"\n",
    "    try:\n",
    "        # Try with pdfplumber first (more reliable)\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Check first few pages\n",
    "            for page_num in range(min(5, len(pdf.pages))):\n",
    "                page = pdf.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                if text and len(text.strip()) > 100:  # Has meaningful text\n",
    "                    return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check your PDF\n",
    "pdf_path = \"pdfs/salesforce_release_notes_1-19-2026.pdf\"\n",
    "has_text = check_pdf_has_text(pdf_path)\n",
    "print(f\"PDF has extractable text: {has_text}\")\n",
    "\n",
    "if has_text:\n",
    "    print(\"✓ Using direct text extraction (much faster!)\")\n",
    "else:\n",
    "    print(\"⚠ PDF doesn't have extractable text - will use image OCR fallback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8823ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text with structure...\n",
      "Processing 842 pages...\n",
      "Created 841 text chunks from 842 pages\n",
      "Extracted 841 text chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Extract Text with Structure\n",
    "def extract_structured_text(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract text from PDF preserving structure (headings, sections).\n",
    "    Returns list of structured chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        print(f\"Processing {len(pdf.pages)} pages...\")\n",
    "        \n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            # Extract text with layout preservation\n",
    "            text = page.extract_text(layout=True)\n",
    "            \n",
    "            if not text or len(text.strip()) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Split into paragraphs/sections\n",
    "            paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "            \n",
    "            # Filter out headers/footers\n",
    "            filtered_paragraphs = []\n",
    "            for para in paragraphs:\n",
    "                para_lower = para.lower()\n",
    "                # Skip common header/footer patterns (only if short)\n",
    "                if any(skip in para_lower for skip in [\n",
    "                    'salesforce release notes', 'page', 'copyright',\n",
    "                    'confidential', 'table of contents'\n",
    "                ]) and len(para) < 100:  # Only skip if it's a short header/footer\n",
    "                    continue\n",
    "                filtered_paragraphs.append(para)\n",
    "            \n",
    "            # Create chunks from paragraphs\n",
    "            # Group consecutive paragraphs into chunks (500-2000 chars each)\n",
    "            current_chunk_text = []\n",
    "            current_chunk_size = 0\n",
    "            \n",
    "            for para in filtered_paragraphs:\n",
    "                para_size = len(para)\n",
    "                \n",
    "                # If adding this para would exceed 2000 chars, save current chunk\n",
    "                if current_chunk_size + para_size > 2000 and current_chunk_text:\n",
    "                    chunk_text = \"\\n\\n\".join(current_chunk_text)\n",
    "                    chunks.append({\n",
    "                        \"page_number\": page_num,\n",
    "                        \"feature_name\": current_chunk_text[0][:100] if current_chunk_text else \"Untitled\",\n",
    "                        \"content\": chunk_text,\n",
    "                        \"text\": chunk_text\n",
    "                    })\n",
    "                    current_chunk_text = [para]\n",
    "                    current_chunk_size = para_size\n",
    "                else:\n",
    "                    current_chunk_text.append(para)\n",
    "                    current_chunk_size += para_size\n",
    "            \n",
    "            # Save last chunk on page if it exists\n",
    "            if current_chunk_text:\n",
    "                chunk_text = \"\\n\\n\".join(current_chunk_text)\n",
    "                chunks.append({\n",
    "                    \"page_number\": page_num,\n",
    "                    \"feature_name\": current_chunk_text[0][:100] if current_chunk_text else \"Untitled\",\n",
    "                    \"content\": chunk_text,\n",
    "                    \"text\": chunk_text\n",
    "                })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks from {len(pdf.pages)} pages\")\n",
    "    return chunks\n",
    "\n",
    "# Extract text if PDF has text\n",
    "if has_text:\n",
    "    print(\"Extracting text with structure...\")\n",
    "    text_chunks = extract_structured_text(pdf_path)\n",
    "    print(f\"Extracted {len(text_chunks)} text chunks\")\n",
    "else:\n",
    "    print(\"Skipping text extraction - will use image processing\")\n",
    "    text_chunks = []\n",
    "\n",
    "# Step 4: GPT Processing for Structured Extraction\n",
    "def extract_release_notes_from_text(chunk: Dict[str, Any], client: OpenAI) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured release notes from text chunk using GPT-4o-mini.\n",
    "    Much faster than image processing!\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are an expert at extracting Salesforce release notes from text.\n",
    "Extract structured information from the provided text chunk.\n",
    "\n",
    "CRITICAL: Do NOT extract content from index pages, table of contents, or navigation pages.\n",
    "These pages typically contain:\n",
    "- Section headings with brief category descriptions (e.g., \"Analytics\", \"Commerce\", \"Customization\")\n",
    "- \"How to use this document\" type instructions\n",
    "- General overview text without specific feature details\n",
    "- Lists of product categories with one-sentence summaries\n",
    "- Pages that say things like \"Learn about...\", \"Read about...\", \"Includes features for...\"\n",
    "- Pages with multiple category headings but no detailed feature descriptions\n",
    "\n",
    "ONLY extract actual release notes that describe specific features, enhancements, or changes with detailed information.\n",
    "A real release note should have:\n",
    "- A specific feature name (not just a category name like \"Analytics\" or \"Commerce\")\n",
    "- A detailed description explaining what the feature does or what changed\n",
    "- Specific technical details, capabilities, or user-facing changes\n",
    "\n",
    "If the text appears to be an index, table of contents, or navigation page with only category summaries, return an empty release_notes array.\n",
    "\n",
    "For each release note, extract ALL of the following information if present:\n",
    "- feature_name: The name of the feature or change\n",
    "- category: High-level category (e.g., Sales Cloud, Service Cloud, Platform, API, Security, etc.)\n",
    "- product_area: Specific product/technology area (e.g., Flow Builder, Lightning Web Components, Apex, REST API, SOQL, Mobile, etc.)\n",
    "- change_type: Type of change (e.g., \"New Feature\", \"Enhancement\", \"Breaking Change\", \"Deprecation\", \"Retirement\", \"Security Update\", \"Bug Fix\", \"Stabilization\")\n",
    "- description: Detailed description of the feature or change (include all important details)\n",
    "- availability: When it's available (e.g., \"Available Now\", \"Pilot\", \"Beta\", specific date)\n",
    "- feature_status: Status/impact level (e.g., \"Beta\", \"Pilot\", \"General Availability\", \"Preview\", \"Release Update\")\n",
    "- activation_required: How to activate (e.g., \"Auto-enabled\", \"Requires Release Update\", \"Manual Activation\", \"Opt-in\", \"N/A\")\n",
    "- edition: Which Salesforce edition(s) it applies to (if mentioned)\n",
    "- api_version: Any API version numbers mentioned (e.g., \"API 59.0\", \"API Version 60.0\", \"LWC API Version 59.0\")\n",
    "- timeline: Important dates (activation date, preview period, sunset date, etc.)\n",
    "- affected_components: List of specific components, classes, modules, or features mentioned\n",
    "- breaking_changes: Details about breaking changes, migration requirements, or compatibility notes (if applicable)\n",
    "- security_notes: Any security-related information or requirements\n",
    "\n",
    "Return a JSON object with this structure:\n",
    "{\n",
    "    \"page_number\": <page number>,\n",
    "    \"release_notes\": [\n",
    "        {\n",
    "            \"feature_name\": \"...\",\n",
    "            \"category\": \"...\",\n",
    "            \"product_area\": \"...\",\n",
    "            \"change_type\": \"...\",\n",
    "            \"description\": \"...\",\n",
    "            \"availability\": \"...\",\n",
    "            \"feature_status\": \"...\",\n",
    "            \"activation_required\": \"...\",\n",
    "            \"edition\": \"...\",\n",
    "            \"api_version\": \"...\",\n",
    "            \"timeline\": \"...\",\n",
    "            \"affected_components\": [\"...\"],\n",
    "            \"breaking_changes\": \"...\",\n",
    "            \"security_notes\": \"...\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "IMPORTANT EXTRACTION GUIDELINES:\n",
    "1. Extract EVERY release note in the text, even if some fields are empty\n",
    "2. If a field is not mentioned, use \"N/A\" or empty string/array as appropriate\n",
    "3. Pay special attention to:\n",
    "   - Breaking changes and deprecations (these are critical)\n",
    "   - API version numbers (especially for LWC and Apex)\n",
    "   - Release update requirements\n",
    "   - Security-related changes\n",
    "4. For \"affected_components\", extract as an array of strings\n",
    "5. Include full descriptions - don't truncate important details\n",
    "6. DO NOT extract category summaries or index page content - only extract actual feature release notes\n",
    "\n",
    "If there are no release notes in the text (e.g., it's an index page), return an empty release_notes array.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Extract release notes from this text:\\n\\n{chunk['text']}\"\n",
    "                }\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        result[\"page_number\"] = chunk.get(\"page_number\", 0)\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk from page {chunk.get('page_number', 'unknown')}: {str(e)}\")\n",
    "        return {\"page_number\": chunk.get(\"page_number\", 0), \"release_notes\": []}\n",
    "\n",
    "# Fallback: Image processing functions (kept for compatibility)\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Encode image to base64 string.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def extract_release_notes(image_path: str, client: OpenAI) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract release notes from an image using GPT-4o-mini.\n",
    "    \n",
    "    Returns structured JSON with release notes.\n",
    "    \"\"\"\n",
    "    # Encode image to base64\n",
    "    base64_image = encode_image_to_base64(image_path)\n",
    "    \n",
    "    # System prompt for structured extraction\n",
    "    system_prompt = \"\"\"You are an expert at extracting Salesforce release notes from PDF pages. \n",
    "Extract all release notes from the image and return them in a structured JSON format.\n",
    "\n",
    "For each release note, extract ALL of the following information if present:\n",
    "- feature_name: The name of the feature or change\n",
    "- category: High-level category (e.g., Sales Cloud, Service Cloud, Platform, API, Security, etc.)\n",
    "- product_area: Specific product/technology area (e.g., Flow Builder, Lightning Web Components, Apex, REST API, SOQL, Mobile, etc.)\n",
    "- change_type: Type of change (e.g., \"New Feature\", \"Enhancement\", \"Breaking Change\", \"Deprecation\", \"Retirement\", \"Security Update\", \"Bug Fix\", \"Stabilization\")\n",
    "- description: Detailed description of the feature or change (include all important details)\n",
    "- availability: When it's available (e.g., \"Available Now\", \"Pilot\", \"Beta\", specific date)\n",
    "- feature_status: Status/impact level (e.g., \"Beta\", \"Pilot\", \"General Availability\", \"Preview\", \"Release Update\")\n",
    "- activation_required: How to activate (e.g., \"Auto-enabled\", \"Requires Release Update\", \"Manual Activation\", \"Opt-in\", \"N/A\")\n",
    "- edition: Which Salesforce edition(s) it applies to (if mentioned)\n",
    "- api_version: Any API version numbers mentioned (e.g., \"API 59.0\", \"API Version 60.0\", \"LWC API Version 59.0\")\n",
    "- timeline: Important dates (activation date, preview period, sunset date, etc.)\n",
    "- affected_components: List of specific components, classes, modules, or features mentioned\n",
    "- breaking_changes: Details about breaking changes, migration requirements, or compatibility notes (if applicable)\n",
    "- security_notes: Any security-related information or requirements\n",
    "\n",
    "Return a JSON object with this structure:\n",
    "{\n",
    "    \"page_number\": <page number>,\n",
    "    \"release_notes\": [\n",
    "        {\n",
    "            \"feature_name\": \"...\",\n",
    "            \"category\": \"...\",\n",
    "            \"product_area\": \"...\",\n",
    "            \"change_type\": \"...\",\n",
    "            \"description\": \"...\",\n",
    "            \"availability\": \"...\",\n",
    "            \"feature_status\": \"...\",\n",
    "            \"activation_required\": \"...\",\n",
    "            \"edition\": \"...\",\n",
    "            \"api_version\": \"...\",\n",
    "            \"timeline\": \"...\",\n",
    "            \"affected_components\": [\"...\"],\n",
    "            \"breaking_changes\": \"...\",\n",
    "            \"security_notes\": \"...\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "IMPORTANT EXTRACTION GUIDELINES:\n",
    "1. Extract EVERY release note on the page, even if some fields are empty\n",
    "2. If a field is not mentioned, use \"N/A\" or empty string/array as appropriate\n",
    "3. Pay special attention to:\n",
    "   - Breaking changes and deprecations (these are critical)\n",
    "   - API version numbers (especially for LWC and Apex)\n",
    "   - Release update requirements\n",
    "   - Security-related changes\n",
    "4. For \"affected_components\", extract as an array of strings\n",
    "5. Include full descriptions - don't truncate important details\n",
    "6. Skip headers, footers, page numbers, and navigation elements\n",
    "7. If multiple related features are grouped together, extract them as separate notes if they have distinct names/descriptions\n",
    "\n",
    "If there are no release notes on the page, return an empty release_notes array.\n",
    "Only extract actual release notes, skip headers, footers, and navigation elements.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Extract all release notes from this page in the specified JSON format.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return {\"page_number\": 0, \"release_notes\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db10e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text chunks with GPT-4o-mini in parallel...\n",
      "Total chunks to process: 841\n",
      "  Processing chunk 50/841...\n",
      "  Processing chunk 100/841...\n",
      "  Processing chunk 150/841...\n",
      "  Processing chunk 200/841...\n",
      "  Processing chunk 250/841...\n",
      "  Processing chunk 300/841...\n",
      "  Processing chunk 350/841...\n",
      "  Processing chunk 400/841...\n",
      "  Processing chunk 450/841...\n",
      "  Processing chunk 500/841...\n",
      "  Processing chunk 550/841...\n",
      "  Processing chunk 600/841...\n",
      "  Processing chunk 650/841...\n",
      "  Processing chunk 700/841...\n",
      "  Processing chunk 750/841...\n",
      "  Processing chunk 800/841...\n",
      "Error processing chunk from page 762: Connection error.\n",
      "\n",
      "Processed 841 chunks\n",
      "\n",
      "Extraction complete!\n",
      "Total chunks/pages processed: 841\n",
      "Pages with valid release notes: 783\n",
      "Total release notes extracted: 3265\n",
      "\n",
      "--- Sample Release Note (Markdown Format) ---\n",
      "Page 6:\n",
      "\n",
      "**Setup with Agentforce (beta)**\n",
      "Category: Customization\n",
      "Product Area: Setup\n",
      "Change Type: New Feature\n",
      "Description: Complete Setup tasks more efficiently with the help of an AI assistant....\n",
      "\n",
      "**Translation Workbench Enhancements**\n",
      "Category: Customization\n",
      "Product Area: Translation Workbench\n",
      "Change Type: Enhancement\n",
      "Description: Import and export translation files more easily with enhancements to the Translation Workbench....\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Process Text Chunks in Parallel (or fallback to images)\n",
    "if has_text and text_chunks:\n",
    "    print(\"Processing text chunks with GPT-4o-mini in parallel...\")\n",
    "    print(f\"Total chunks to process: {len(text_chunks)}\")\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    def process_chunk_with_progress(chunk_idx_chunk):\n",
    "        idx, chunk = chunk_idx_chunk\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processing chunk {idx + 1}/{len(text_chunks)}...\")\n",
    "        return extract_release_notes_from_text(chunk, client)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        all_extracted_data = list(executor.map(\n",
    "            process_chunk_with_progress,\n",
    "            enumerate(text_chunks)\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\nProcessed {len(all_extracted_data)} chunks\")\n",
    "    \n",
    "else:\n",
    "    # Fallback: Image processing\n",
    "    print(\"Using image processing fallback...\")\n",
    "    from pdf2image import convert_from_path\n",
    "    \n",
    "    def convert_pdf_to_images(pdf_path: str, output_folder: str = \"images\") -> List[str]:\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "        pdf_name = Path(pdf_path).stem\n",
    "        images = convert_from_path(pdf_path)\n",
    "        image_paths = []\n",
    "        for page_num, image in enumerate(images, start=1):\n",
    "            image_filename = f\"{pdf_name}_page_{page_num}.jpg\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            image.save(image_path, \"JPEG\")\n",
    "            image_paths.append(image_path)\n",
    "        return image_paths\n",
    "    \n",
    "    def process_all_images_parallel(image_paths: List[str], client: OpenAI, max_workers: int = 5) -> List[Dict[str, Any]]:\n",
    "        all_notes = []\n",
    "        from concurrent.futures import as_completed\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_path = {\n",
    "                executor.submit(extract_release_notes, path, client): path\n",
    "                for path in image_paths\n",
    "            }\n",
    "            for future in as_completed(future_to_path):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    all_notes.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "                    all_notes.append({\"page_number\": 0, \"release_notes\": []})\n",
    "        all_notes.sort(key=lambda x: x.get(\"page_number\", 0))\n",
    "        return all_notes\n",
    "    \n",
    "    image_paths = convert_pdf_to_images(pdf_path)\n",
    "    all_extracted_data = process_all_images_parallel(image_paths, client, max_workers=5)\n",
    "\n",
    "# Filter out pages with no valid release notes\n",
    "filtered_data = []\n",
    "for page_data in all_extracted_data:\n",
    "    notes = page_data.get(\"release_notes\", [])\n",
    "    # Filter: keep only pages with at least one note that has meaningful content\n",
    "    valid_notes = []\n",
    "    for note in notes:\n",
    "        feature_name = note.get(\"feature_name\", \"\").strip()\n",
    "        description = note.get(\"description\", \"\").strip()\n",
    "        \n",
    "        # Basic validation\n",
    "        if not feature_name or not description:\n",
    "            continue\n",
    "        \n",
    "        # Filter out category/section descriptions (common in index pages)\n",
    "        if len(description) < 50:  # Too short to be a real feature description\n",
    "            continue\n",
    "        \n",
    "        # Filter out notes that look like section headers\n",
    "        skip_patterns = [\n",
    "            \"learn about\", \"read about\", \"includes features for\", \n",
    "            \"enhancements include\", \"provides\", \"offers\",\n",
    "            \"check back\", \"see\", \"review\", \"find\"\n",
    "        ]\n",
    "        description_lower = description.lower()\n",
    "        if any(pattern in description_lower[:100] for pattern in skip_patterns):\n",
    "            if len(description) < 150:\n",
    "                continue\n",
    "        \n",
    "        valid_notes.append(note)\n",
    "    \n",
    "    if valid_notes:\n",
    "        page_data[\"release_notes\"] = valid_notes\n",
    "        filtered_data.append(page_data)\n",
    "\n",
    "print(f\"\\nExtraction complete!\")\n",
    "print(f\"Total chunks/pages processed: {len(all_extracted_data)}\")\n",
    "print(f\"Pages with valid release notes: {len(filtered_data)}\")\n",
    "\n",
    "# Count total notes\n",
    "total_notes = sum(len(page.get(\"release_notes\", [])) for page in filtered_data)\n",
    "print(f\"Total release notes extracted: {total_notes}\")\n",
    "\n",
    "# Display sample in markdown format for readability\n",
    "if filtered_data:\n",
    "    print(\"\\n--- Sample Release Note (Markdown Format) ---\")\n",
    "    sample_page = filtered_data[0]\n",
    "    print(f\"Page {sample_page.get('page_number', 'N/A')}:\")\n",
    "    for note in sample_page.get(\"release_notes\", [])[:2]:  # Show first 2 notes\n",
    "        print(f\"\\n**{note.get('feature_name', 'N/A')}**\")\n",
    "        print(f\"Category: {note.get('category', 'N/A')}\")\n",
    "        print(f\"Product Area: {note.get('product_area', 'N/A')}\")\n",
    "        print(f\"Change Type: {note.get('change_type', 'N/A')}\")\n",
    "        print(f\"Description: {note.get('description', 'N/A')[:200]}...\")\n",
    "        if note.get('breaking_changes') and note.get('breaking_changes') != 'N/A':\n",
    "            print(f\"⚠️ Breaking Changes: {note.get('breaking_changes', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9587dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_images(image_paths: List[str], client: OpenAI) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process all images and extract release notes.\"\"\"\n",
    "    all_notes = []\n",
    "    \n",
    "    for idx, image_path in enumerate(image_paths, start=1):\n",
    "        print(f\"Processing page {idx}/{len(image_paths)}: {os.path.basename(image_path)}\")\n",
    "        result = extract_release_notes(image_path, client)\n",
    "        \n",
    "        # Add page number if not present\n",
    "        if \"page_number\" not in result:\n",
    "            result[\"page_number\"] = idx\n",
    "        \n",
    "        # Add image path for reference\n",
    "        result[\"image_path\"] = image_path\n",
    "        \n",
    "        all_notes.append(result)\n",
    "        \n",
    "        # Print summary\n",
    "        num_notes = len(result.get(\"release_notes\", []))\n",
    "        print(f\"  Extracted {num_notes} release note(s)\")\n",
    "    \n",
    "    return all_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e6e4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Processing is handled in Cell 3 above\n",
    "# This cell is intentionally left empty - all extraction and filtering happens in Cell 3\n",
    "# which handles both text extraction (if PDF has text) and image processing (fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81672f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered release notes to filtered_release_notes.json\n",
      "Generating embeddings for 3265 release notes (parallel processing with 10 workers)...\n",
      "  Processed 50/3265 embeddings...\n",
      "  Processed 100/3265 embeddings...\n",
      "  Processed 150/3265 embeddings...\n",
      "  Processed 200/3265 embeddings...\n",
      "  Processed 250/3265 embeddings...\n",
      "  Processed 300/3265 embeddings...\n",
      "  Processed 350/3265 embeddings...\n",
      "  Processed 400/3265 embeddings...\n",
      "  Processed 450/3265 embeddings...\n",
      "  Processed 500/3265 embeddings...\n",
      "  Processed 550/3265 embeddings...\n",
      "  Processed 600/3265 embeddings...\n",
      "  Processed 650/3265 embeddings...\n",
      "  Processed 700/3265 embeddings...\n",
      "  Processed 750/3265 embeddings...\n",
      "  Processed 800/3265 embeddings...\n",
      "  Processed 850/3265 embeddings...\n",
      "  Processed 900/3265 embeddings...\n",
      "  Processed 950/3265 embeddings...\n",
      "  Processed 1000/3265 embeddings...\n",
      "  Processed 1050/3265 embeddings...\n",
      "  Processed 1100/3265 embeddings...\n",
      "  Processed 1150/3265 embeddings...\n",
      "  Processed 1200/3265 embeddings...\n",
      "  Processed 1250/3265 embeddings...\n",
      "  Processed 1300/3265 embeddings...\n",
      "  Processed 1350/3265 embeddings...\n",
      "  Processed 1400/3265 embeddings...\n",
      "  Processed 1450/3265 embeddings...\n",
      "  Processed 1500/3265 embeddings...\n",
      "  Processed 1550/3265 embeddings...\n",
      "  Processed 1600/3265 embeddings...\n",
      "  Processed 1650/3265 embeddings...\n",
      "  Processed 1700/3265 embeddings...\n",
      "  Processed 1750/3265 embeddings...\n",
      "  Processed 1800/3265 embeddings...\n",
      "  Processed 1850/3265 embeddings...\n",
      "  Processed 1900/3265 embeddings...\n",
      "  Processed 1950/3265 embeddings...\n",
      "  Processed 2000/3265 embeddings...\n",
      "  Processed 2050/3265 embeddings...\n",
      "  Processed 2100/3265 embeddings...\n",
      "  Processed 2150/3265 embeddings...\n",
      "  Processed 2200/3265 embeddings...\n",
      "  Processed 2250/3265 embeddings...\n",
      "  Processed 2300/3265 embeddings...\n",
      "  Processed 2350/3265 embeddings...\n",
      "  Processed 2400/3265 embeddings...\n",
      "  Processed 2450/3265 embeddings...\n",
      "  Processed 2500/3265 embeddings...\n",
      "  Processed 2550/3265 embeddings...\n",
      "  Processed 2600/3265 embeddings...\n",
      "  Processed 2650/3265 embeddings...\n",
      "  Processed 2700/3265 embeddings...\n",
      "  Processed 2750/3265 embeddings...\n",
      "  Processed 2800/3265 embeddings...\n",
      "  Processed 2850/3265 embeddings...\n",
      "  Processed 2900/3265 embeddings...\n",
      "  Processed 2950/3265 embeddings...\n",
      "  Processed 3000/3265 embeddings...\n",
      "  Processed 3050/3265 embeddings...\n",
      "  Processed 3100/3265 embeddings...\n",
      "  Processed 3150/3265 embeddings...\n",
      "  Processed 3200/3265 embeddings...\n",
      "  Processed 3250/3265 embeddings...\n",
      "\n",
      "Embedding matrix shape: (3265, 3072)\n",
      "Expected shape: (n_notes, 3072) for text-embedding-3-large\n",
      "Data type: float32\n",
      "\n",
      "Sample embedding (first 10 values): [-0.00286061 -0.04232433 -0.01020952 -0.03374462 -0.00917928  0.03431885\n",
      "  0.02090883  0.0451955   0.00742703  0.04631019]\n",
      "Embedding min value: -0.126207\n",
      "Embedding max value: 0.161377\n",
      "Embedding mean value: 0.000018\n",
      "\n",
      "Embedding generation complete! Generated 3265 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Embedding Generation\n",
    "# Ensure as_completed is imported\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "# Save filtered release notes to JSON\n",
    "with open(\"filtered_release_notes.json\", \"w\") as f:\n",
    "    json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "print(\"Saved filtered release notes to filtered_release_notes.json\")\n",
    "\n",
    "def generate_embeddings(notes_data: List[Dict[str, Any]], client: OpenAI, max_workers: int = 10) -> tuple:\n",
    "    \"\"\"\n",
    "    Generate embeddings for release notes using text-embedding-3-large with parallel processing.\n",
    "    \n",
    "    Args:\n",
    "        notes_data: List of page data with release notes\n",
    "        client: OpenAI client\n",
    "        max_workers: Number of parallel workers (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (numpy array of embeddings, list of note items)\n",
    "    \"\"\"\n",
    "    # Flatten all notes into a list\n",
    "    all_notes = []\n",
    "    for page_data in notes_data:\n",
    "        for note in page_data.get(\"release_notes\", []):\n",
    "            # Combine multiple fields for richer embedding\n",
    "            text_parts = [\n",
    "                note.get('feature_name', ''),\n",
    "                note.get('category', ''),\n",
    "                note.get('product_area', ''),\n",
    "                note.get('change_type', ''),\n",
    "                note.get('description', ''),\n",
    "                note.get('availability', ''),\n",
    "                note.get('feature_status', ''),\n",
    "                note.get('api_version', ''),\n",
    "                note.get('timeline', ''),\n",
    "            ]\n",
    "            # Add breaking changes and security notes if present\n",
    "            if note.get('breaking_changes') and note.get('breaking_changes') != 'N/A':\n",
    "                text_parts.append(f\"Breaking Changes: {note.get('breaking_changes')}\")\n",
    "            if note.get('security_notes') and note.get('security_notes') != 'N/A':\n",
    "                text_parts.append(f\"Security: {note.get('security_notes')}\")\n",
    "            # Add affected components if present\n",
    "            if note.get('affected_components') and note.get('affected_components') != []:\n",
    "                components_str = ', '.join(note.get('affected_components', []))\n",
    "                text_parts.append(f\"Affected Components: {components_str}\")\n",
    "            \n",
    "            text_to_embed = \" \".join([part for part in text_parts if part and part != 'N/A']).strip()\n",
    "            if text_to_embed:\n",
    "                all_notes.append({\n",
    "                    \"text\": text_to_embed,\n",
    "                    \"note\": note,\n",
    "                    \"page_data\": page_data\n",
    "                })\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(all_notes)} release notes (parallel processing with {max_workers} workers)...\")\n",
    "    \n",
    "    # Function to generate a single embedding\n",
    "    def generate_single_embedding(note_item):\n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                input=note_item[\"text\"]\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding note: {str(e)}\")\n",
    "            # Return zero vector as fallback\n",
    "            return [0.0] * 3072\n",
    "    \n",
    "    # Generate embeddings in parallel\n",
    "    embeddings = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_idx = {\n",
    "            executor.submit(generate_single_embedding, note_item): idx\n",
    "            for idx, note_item in enumerate(all_notes)\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete (maintain order)\n",
    "        results = [None] * len(all_notes)\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                results[idx] = future.result()\n",
    "                completed += 1\n",
    "                if completed % 50 == 0:\n",
    "                    print(f\"  Processed {completed}/{len(all_notes)} embeddings...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting embedding result: {str(e)}\")\n",
    "                results[idx] = [0.0] * 3072\n",
    "        \n",
    "        embeddings = results\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings_array = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    return embeddings_array, all_notes\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings_matrix, notes_list = generate_embeddings(filtered_data, client)\n",
    "\n",
    "# Verify embedding matrix shape\n",
    "print(f\"\\nEmbedding matrix shape: {embeddings_matrix.shape}\")\n",
    "print(f\"Expected shape: (n_notes, 3072) for text-embedding-3-large\")\n",
    "print(f\"Data type: {embeddings_matrix.dtype}\")\n",
    "\n",
    "# Display sample values\n",
    "print(f\"\\nSample embedding (first 10 values): {embeddings_matrix[0][:10]}\")\n",
    "print(f\"Embedding min value: {embeddings_matrix.min():.6f}\")\n",
    "print(f\"Embedding max value: {embeddings_matrix.max():.6f}\")\n",
    "print(f\"Embedding mean value: {embeddings_matrix.mean():.6f}\")\n",
    "\n",
    "print(f\"\\nEmbedding generation complete! Generated {len(embeddings_matrix)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6b129c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index with dimension: 3072\n",
      "Adding 3265 vectors to index...\n",
      "Index contains 3265 vectors\n",
      "Index saved to filtered_sf_release_notes_index.index\n",
      "Metadata saved to metadata_json.json\n",
      "Metadata contains 3265 notes and 3265 image paths\n",
      "\n",
      "FAISS index creation complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: FAISS Index Creation\n",
    "# Get embedding dimension\n",
    "embedding_dim = embeddings_matrix.shape[1]\n",
    "print(f\"Creating FAISS index with dimension: {embedding_dim}\")\n",
    "\n",
    "# Create FAISS index (L2 distance)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add vectors to the index\n",
    "print(f\"Adding {len(embeddings_matrix)} vectors to index...\")\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "# Verify index contains correct number of vectors\n",
    "print(f\"Index contains {index.ntotal} vectors\")\n",
    "assert index.ntotal == len(embeddings_matrix), \"Index vector count mismatch!\"\n",
    "\n",
    "# Save index\n",
    "index_filename = \"filtered_sf_release_notes_index.index\"\n",
    "faiss.write_index(index, index_filename)\n",
    "print(f\"Index saved to {index_filename}\")\n",
    "\n",
    "# Create metadata file\n",
    "metadata = {\n",
    "    \"notes\": [note_item[\"note\"] for note_item in notes_list],\n",
    "    \"image_paths\": [note_item[\"page_data\"].get(\"image_path\", \"\") for note_item in notes_list]\n",
    "}\n",
    "\n",
    "metadata_filename = \"metadata_json.json\"\n",
    "with open(metadata_filename, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to {metadata_filename}\")\n",
    "print(f\"Metadata contains {len(metadata['notes'])} notes and {len(metadata['image_paths'])} image paths\")\n",
    "\n",
    "print(\"\\nFAISS index creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc5613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query function...\n",
      "Generating embedding for query: 'What are the new features in Sales Cloud?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "--- Sample Query Results ---\n",
      "\n",
      "Rank 1 (Distance: 0.8084)\n",
      "Feature: Revenue Cloud for Communications on Salesforce Platform\n",
      "Category: Industries\n",
      "Change Type: New Feature\n",
      "Description: Discover, set up, and configure Revenue Cloud for Communications features with Salesforce Go. Boost ...\n",
      "\n",
      "Rank 2 (Distance: 0.8220)\n",
      "Feature: AI-Powered Selling Initial Setup\n",
      "Category: Sales\n",
      "Change Type: New Feature\n",
      "Description: AI-Powered Selling Initial Setup simplifies configuration for new and existing orgs. It automaticall...\n",
      "\n",
      "Rank 3 (Distance: 0.8383)\n",
      "Feature: Get Your Agentic Sales Org Ready in a Few Clicks\n",
      "Category: Sales Cloud\n",
      "Change Type: New Feature\n",
      "Description: This feature simplifies the process of preparing your Agentic Sales Organization with just a few cli...\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Query Function\n",
    "def query_embeddings(query: str, index: faiss.Index, metadata: Dict[str, Any], \n",
    "                     client: OpenAI, k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate embedding for query and search FAISS index for top-k similar notes.\n",
    "    \n",
    "    Args:\n",
    "        query: User query string\n",
    "        index: FAISS index\n",
    "        metadata: Metadata dictionary with 'notes' and 'image_paths'\n",
    "        client: OpenAI client\n",
    "        k: Number of top results to return (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing note information, distances, and indices\n",
    "    \"\"\"\n",
    "    # Generate embedding for query\n",
    "    print(f\"Generating embedding for query: '{query}'\")\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = np.array([response.data[0].embedding], dtype=np.float32)\n",
    "    \n",
    "    # Search FAISS index\n",
    "    print(f\"Searching index for top-{k} results...\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Extract top-k similarities\n",
    "    results = []\n",
    "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(metadata[\"notes\"]):\n",
    "            result = {\n",
    "                \"rank\": i + 1,\n",
    "                \"distance\": float(distance),\n",
    "                \"index\": int(idx),\n",
    "                \"note\": metadata[\"notes\"][idx],\n",
    "                \"image_path\": metadata[\"image_paths\"][idx] if idx < len(metadata[\"image_paths\"]) else \"\"\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    print(f\"Found {len(results)} results\")\n",
    "    return results\n",
    "\n",
    "# Test the query function\n",
    "test_query = \"What are the new features in Sales Cloud?\"\n",
    "print(\"Testing query function...\")\n",
    "test_results = query_embeddings(test_query, index, metadata, client, k=5)\n",
    "\n",
    "print(\"\\n--- Sample Query Results ---\")\n",
    "for result in test_results[:3]:  # Show first 3\n",
    "    print(f\"\\nRank {result['rank']} (Distance: {result['distance']:.4f})\")\n",
    "    print(f\"Feature: {result['note'].get('feature_name', 'N/A')}\")\n",
    "    print(f\"Category: {result['note'].get('category', 'N/A')}\")\n",
    "    print(f\"Change Type: {result['note'].get('change_type', 'N/A')}\")\n",
    "    print(f\"Description: {result['note'].get('description', 'N/A')[:100]}...\")\n",
    "    if result['note'].get('breaking_changes') and result['note'].get('breaking_changes') != 'N/A':\n",
    "        print(f\"⚠️ Breaking Changes: {result['note'].get('breaking_changes', 'N/A')[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fae5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG pipeline...\n",
      "Generating embedding for query: 'What are the new features in Sales Cloud?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "Sending query to gpt-4o-mini...\n",
      "\n",
      "============================================================\n",
      "RAG Query Result:\n",
      "============================================================\n",
      "The new features in Sales Cloud based on the release notes are:\n",
      "\n",
      "1. **AI-Powered Selling Initial Setup**: This feature simplifies the configuration for new and existing organizations by automatically turning on base Sales features and agents, and assigning the correct permissions to sellers. It provides a consistent, ready-to-use sales environment immediately. Availability is set for December 2025, and it requires manual activation.\n",
      "\n",
      "2. **Sales Workspace**: This feature offers sellers a consolidated workspace that provides clarity on goals, key metrics, AI-driven recommendations, and actionable insights. It replaces the traditional Seller Home to enhance the daily selling experience. Like the AI-Powered Selling Initial Setup, it will be available in December 2025 and requires manual activation.\n",
      "\n",
      "Additionally, there is a feature called **Get Your Agentic Sales Org Ready in a Few Clicks**, which simplifies the preparation of an Agentic Sales Organization and is available now with auto-enabled activation. \n",
      "\n",
      "These features aim to enhance the sales experience and streamline processes for sales teams.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 7: RAG Pipeline\n",
    "def combine_results_for_gpt(results: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Combine all retrieved notes into a formatted string for GPT context.\n",
    "    \n",
    "    Args:\n",
    "        results: List of result dictionaries from query_embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with all notes\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    for result in results:\n",
    "        note = result[\"note\"]\n",
    "        context = f\"Feature: {note.get('feature_name', 'N/A')}\\n\"\n",
    "        context += f\"Category: {note.get('category', 'N/A')}\\n\"\n",
    "        context += f\"Product Area: {note.get('product_area', 'N/A')}\\n\"\n",
    "        context += f\"Change Type: {note.get('change_type', 'N/A')}\\n\"\n",
    "        context += f\"Description: {note.get('description', 'N/A')}\\n\"\n",
    "        context += f\"Availability: {note.get('availability', 'N/A')}\\n\"\n",
    "        context += f\"Feature Status: {note.get('feature_status', 'N/A')}\\n\"\n",
    "        context += f\"Activation Required: {note.get('activation_required', 'N/A')}\\n\"\n",
    "        context += f\"Edition: {note.get('edition', 'N/A')}\\n\"\n",
    "        context += f\"API Version: {note.get('api_version', 'N/A')}\\n\"\n",
    "        context += f\"Timeline: {note.get('timeline', 'N/A')}\\n\"\n",
    "        if note.get('affected_components'):\n",
    "            context += f\"Affected Components: {', '.join(note.get('affected_components', []))}\\n\"\n",
    "        if note.get('breaking_changes') and note.get('breaking_changes') != 'N/A':\n",
    "            context += f\"⚠️ Breaking Changes: {note.get('breaking_changes', 'N/A')}\\n\"\n",
    "        if note.get('security_notes') and note.get('security_notes') != 'N/A':\n",
    "            context += f\"🔒 Security Notes: {note.get('security_notes', 'N/A')}\\n\"\n",
    "        context += \"---\"\n",
    "        context_parts.append(context)\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "def create_gpt_prompt(query: str, context: str, system_prompt: str = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create system and user prompts for GPT.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        context: Combined context from retrieved notes\n",
    "        system_prompt: Optional custom system prompt\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'system' and 'user' keys\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"You are a helpful assistant that answers questions about Salesforce release notes.\n",
    "Use the provided release notes context to answer the user's question accurately.\n",
    "If the context doesn't contain relevant information, say so.\n",
    "Be concise but comprehensive in your answers.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Based on the following Salesforce release notes, please answer the question.\n",
    "\n",
    "Release Notes Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a clear and helpful answer based on the release notes above.\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"system\": system_prompt,\n",
    "        \"user\": user_prompt\n",
    "    }\n",
    "\n",
    "def rag_query(user_query: str, index: faiss.Index, metadata: Dict[str, Any], \n",
    "              client: OpenAI, k: int = 5, model: str = \"gpt-4o-mini\", \n",
    "              system_prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: query embedding, retrieval, and GPT response.\n",
    "    \n",
    "    Args:\n",
    "        user_query: User's question\n",
    "        index: FAISS index\n",
    "        metadata: Metadata dictionary\n",
    "        client: OpenAI client\n",
    "        k: Number of top results to retrieve (default: 5)\n",
    "        model: GPT model to use (default: \"gpt-4o-mini\")\n",
    "        system_prompt: Optional custom system prompt\n",
    "    \n",
    "    Returns:\n",
    "        GPT's response as a string\n",
    "    \"\"\"\n",
    "    # Step 1: Query embeddings and retrieve top-k results\n",
    "    results = query_embeddings(user_query, index, metadata, client, k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant release notes found for your query.\"\n",
    "    \n",
    "    # Step 2: Combine results for GPT\n",
    "    context = combine_results_for_gpt(results)\n",
    "    \n",
    "    # Step 3: Create GPT prompt\n",
    "    prompts = create_gpt_prompt(user_query, context, system_prompt)\n",
    "    \n",
    "    # Step 4: Send to GPT API\n",
    "    print(f\"\\nSending query to {model}...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompts[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"user\"]}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Extract and return GPT's response\n",
    "    gpt_response = response.choices[0].message.content\n",
    "    return gpt_response\n",
    "\n",
    "# Test the RAG pipeline\n",
    "print(\"Testing RAG pipeline...\")\n",
    "test_rag_query = \"What are the new features in Sales Cloud?\"\n",
    "rag_response = rag_query(test_rag_query, index, metadata, client, k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG Query Result:\")\n",
    "print(\"=\"*60)\n",
    "print(rag_response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8465894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved index and metadata...\n",
      "Loaded index with 3265 vectors\n",
      "Loaded metadata with 3265 notes\n",
      "\n",
      "================================================================================\n",
      "Running Sample Queries\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query 1: What changes were made to Flow Builder?\n",
      "================================================================================\n",
      "Generating embedding for query: 'What changes were made to Flow Builder?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "Sending query to gpt-4o-mini...\n",
      "\n",
      "Answer:\n",
      "The recent updates to Flow Builder include several enhancements:\n",
      "\n",
      "1. **Natural Language Changes**: Users can now make changes to record-triggered and scheduled-triggered flows using natural language.\n",
      "\n",
      "2. **Navigation Improvements**: The Flow Builder canvas has become easier to navigate with collapsible branching paths and more intuitive mouse scrolling controls.\n",
      "\n",
      "3. **Agentforce Panel Access**: Users can access the Agentforce panel without requiring admin setup.\n",
      "\n",
      "4. **Data Table Screen Component**: Enhancements allow for sorting and editing records within the Data Table screen component.\n",
      "\n",
      "5. **Screen Flow Customization**: Users can customize screen flows with component-level style overrides.\n",
      "\n",
      "6. **Asynchronous Broadcast Flows**: Users can send messages to large audiences and utilize Wait elements in asynchronous broadcast flows.\n",
      "\n",
      "7. **Flow Triggering Options**: New options for decisions, exit rules, reentry conditions, and more have been added for activation-triggered flows.\n",
      "\n",
      "8. **Marketing Cloud Integration**: Users can send Marketing Cloud Engagement emails in various flow types, including segment-triggered, event-triggered, activation-triggered, and broadcast flows.\n",
      "\n",
      "9. **Orchestration Improvements**: Orchestration designers can now provide input values and setup options once per session, and debug data is preserved when switching between modes.\n",
      "\n",
      "10. **Email Notification Settings**: New settings allow for reduced email volume from approval and orchestration work item notifications, limiting them to queue members only.\n",
      "\n",
      "These enhancements are aimed at improving usability, customization, and integration capabilities within Flow Builder. All features are currently available and auto-enabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Query 2: What are the new features in Sales Cloud?\n",
      "================================================================================\n",
      "Generating embedding for query: 'What are the new features in Sales Cloud?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "Sending query to gpt-4o-mini...\n",
      "\n",
      "Answer:\n",
      "The new features in Sales Cloud based on the release notes are:\n",
      "\n",
      "1. **AI-Powered Selling Initial Setup**: This feature simplifies configuration for new and existing organizations by automatically turning on base Sales features and agents, and assigning the correct permissions to sellers. It provides a consistent, ready-to-use sales environment immediately. Availability is set for December 2025 and requires manual activation.\n",
      "\n",
      "2. **Sales Workspace**: This feature offers sellers a consolidated workspace that provides clarity on their goals, key metrics, AI-driven recommendations, and actionable insights, replacing the traditional Seller Home. It aims to enhance the daily selling experience. Availability is also set for December 2025 and requires manual activation.\n",
      "\n",
      "Additionally, there is a feature called **Get Your Agentic Sales Org Ready in a Few Clicks**, which simplifies the process of preparing your Agentic Sales Organization with just a few clicks and is available now, auto-enabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Query 3: Are there any API changes in this release?\n",
      "================================================================================\n",
      "Generating embedding for query: 'Are there any API changes in this release?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "Sending query to gpt-4o-mini...\n",
      "\n",
      "Answer:\n",
      "Yes, there are API changes in this release. Specifically, the Platform REST API has new, changed, or deprecated calls in version 66.0. Additionally, there are enhancements related to updating the API version for Lightning web components, which allows developers to utilize new features and improvements while ensuring existing components remain unaffected. This versioning applies to custom Lightning web components in various environments, including Lightning Experience and the Salesforce mobile app.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Query 4: What security features were added?\n",
      "================================================================================\n",
      "Generating embedding for query: 'What security features were added?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "Sending query to gpt-4o-mini...\n",
      "\n",
      "Answer:\n",
      "Based on the release notes, the following security features were added:\n",
      "\n",
      "1. **New Configurable Settings Added to Health Check**: This enhancement allows for deeper insights by tracking seven new configurable security settings, including MFA status, SAML enablement, and session management controls. This feature is available now and requires manual activation for Enterprise, Performance, Unlimited, and Developer editions.\n",
      "\n",
      "2. **Security Alerts for Device Flow**: This enhancement introduces security alerts during the device flow, prompting users to enter a code from their device or app. Alerts have been added to the pages for code entry, successful connection, and denied connection, as well as updates to the OAuth flow authorization page. This feature is available now and is auto-enabled.\n",
      "\n",
      "These features enhance security by providing better tracking and alerts related to user authentication and security settings.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Query 5: What improvements were made to reporting?\n",
      "================================================================================\n",
      "Generating embedding for query: 'What improvements were made to reporting?'\n",
      "Searching index for top-5 results...\n",
      "Found 5 results\n",
      "\n",
      "Sending query to gpt-4o-mini...\n",
      "\n",
      "Answer:\n",
      "Several improvements were made to reporting in the latest Salesforce release notes:\n",
      "\n",
      "1. **Improved Reporting Tools for Marketing**: Enhanced dashboards for mobile app performance and message deliverability were introduced, along with a new Conversion Analytics dashboard to measure attribution and outcomes.\n",
      "\n",
      "2. **Lightning Reports and Dashboards Enhancements**: Users can now save time and reduce errors by retaining existing report settings when adding tables to dashboards. Additionally, custom disclaimers can be added to exported reports to ensure policy and regulatory compliance.\n",
      "\n",
      "3. **Expanded Reporting Capabilities**: More row-level formulas are now allowed, enhancing overall reporting capabilities.\n",
      "\n",
      "4. **Accessibility Enhancements in Analytics**: Various improvements were made to make analytics more accessible, including better keyboard navigation, improved screen reader support, and enhancements to visual elements for users with low vision.\n",
      "\n",
      "5. **Consolidated Reporting for Organizations**: A new feature allows organizations to generate consolidated reports that include environmental, social, and governance metrics from parent companies and subsidiaries, streamlining the reporting process.\n",
      "\n",
      "These enhancements collectively improve the functionality, usability, and accessibility of reporting tools within Salesforce.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Demo - Load Index and Run Sample Queries\n",
    "# Load the saved FAISS index and metadata\n",
    "print(\"Loading saved index and metadata...\")\n",
    "loaded_index = faiss.read_index(\"filtered_sf_release_notes_index.index\")\n",
    "print(f\"Loaded index with {loaded_index.ntotal} vectors\")\n",
    "\n",
    "with open(\"metadata_json.json\", \"r\") as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "print(f\"Loaded metadata with {len(loaded_metadata['notes'])} notes\")\n",
    "\n",
    "# Sample queries to test the complete pipeline\n",
    "sample_queries = [\n",
    "    \"What changes were made to Flow Builder?\",\n",
    "    \"What are the new features in Sales Cloud?\",\n",
    "    \"Are there any API changes in this release?\",\n",
    "    \"What security features were added?\",\n",
    "    \"What improvements were made to reporting?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running Sample Queries\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, query in enumerate(sample_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    response = rag_query(query, loaded_index, loaded_metadata, client, k=5)\n",
    "    print(f\"\\nAnswer:\\n{response}\")\n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cf49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
